import os
import numpy as np
from Application.resnet import mean_extract_image_features,mean_extract_image_features_batch_1 # <-- import trá»±c tiáº¿p tá»« file báº¡n cÃ³
from Application.cluster import build_clusters,build_cluster_faiss  # <-- import trá»±c tiáº¿p tá»« file báº¡n cÃ³
# =============== Cáº¥u hÃ¬nh cÆ¡ báº£n ===============
import hash_table_py as HashTable
import simhash_py as SimHash
import minhash_py as MinHash
import bloom_filter_py as BloomFilter
import os
from collections import defaultdict, Counter
import time
from statistics import mean
from tqdm import tqdm


def evaluate_by_image(base_folder):
    # === BÆ°á»›c 1: Duyá»‡t toÃ n bá»™ thÆ° má»¥c group ===
    group_classes = defaultdict(lambda: defaultdict(int))
    all_classes = set()

    for group_name in sorted(os.listdir(base_folder)):
        group_path = os.path.join(base_folder, group_name)
        if not os.path.isdir(group_path):
            continue

        for fname in os.listdir(group_path):
            if not fname.lower().endswith(('.jpg', '.png', '.jpeg')):
                continue

            class_name = fname.split('_')[0]
            group_classes[group_name][class_name] += 1
            all_classes.add(class_name)

    # === BÆ°á»›c 2: TÃ¬m group cÃ³ sá»‘ áº£nh lá»›n nháº¥t cho má»—i class ===
        class_group_candidates = {}
        for cls in all_classes:
            max_count = 0
            candidates = []

            for group_name, cls_count_dict in group_classes.items():
                count = cls_count_dict.get(cls, 0)
                if count == 0:
                    continue

                # âœ… Kiá»ƒm tra xem class nÃ y cÃ³ pháº£i class cÃ³ nhiá»u áº£nh nháº¥t trong group khÃ´ng
                group_max_count = max(cls_count_dict.values())
                if count < group_max_count:
                    continue  # class nÃ y khÃ´ng thá»‘ng trá»‹ group, bá» qua group nÃ y

                # âœ… Náº¿u lÃ  class máº¡nh nháº¥t group, xÃ©t bÃ¬nh thÆ°á»ng
                if count > max_count:
                    max_count = count
                    candidates = [group_name]
                elif count == max_count:
                    candidates.append(group_name)

            class_group_candidates[cls] = sorted(candidates) if max_count > 0 else []


    # === BÆ°á»›c 3: GÃ¡n táº¡m group Ä‘áº§u tiÃªn cho má»—i class ===
    group_max = {}
    group_owners = defaultdict(list)

    for cls, groups in class_group_candidates.items():
        if groups:
            group_max[cls] = groups[0]
            group_owners[groups[0]].append(cls)
        else:
            group_max[cls] = None

    # === BÆ°á»›c 4: Xá»­ lÃ½ xung Ä‘á»™t ===
    changed = True
    while changed:
        changed = False
        for group_name, cls_list in list(group_owners.items()):
            if len(cls_list) <= 1:
                continue

            cls_list.sort(key=lambda c: (-group_classes[group_name][c], c))
            winner = cls_list[0]
            losers = cls_list[1:]
            group_owners[group_name] = [winner]

            for loser in losers:
                changed = True
                old_groups = class_group_candidates[loser]
                new_group = None

                for g in old_groups:
                    if g == group_name:
                        continue
                    count_in_g = group_classes[g].get(loser, 0)
                    max_count_for_loser = max(cls_count.get(loser, 0) for cls_count in group_classes.values())
                    if count_in_g < max_count_for_loser:
                        continue
                    if g in group_owners and len(group_owners[g]) == 1:
                        current_owner = group_owners[g][0]
                        c1 = group_classes[g][loser]
                        c2 = group_classes[g][current_owner]
                        if c1 == c2 and loser > current_owner:
                            continue

                    if loser not in group_owners[g]:
                        new_group = g
                        break

                if new_group:
                    group_max[loser] = new_group
                    group_owners[new_group].append(loser)
                else:
                    group_max[loser] = None  # âŒ KhÃ´ng cÃ³ group há»£p lá»‡

    # === BÆ°á»›c 5: TÃ­nh Ä‘á»™ chÃ­nh xÃ¡c ===
    total_images = 0
    correct = 0
    wrong = 0

    for group_name in sorted(os.listdir(base_folder)):
        group_path = os.path.join(base_folder, group_name)
        if not os.path.isdir(group_path):
            continue

        for fname in os.listdir(group_path):
            if not fname.lower().endswith(('.jpg', '.png', '.jpeg')):
                continue

            total_images += 1
            class_name = fname.split('_')[0]
            expected_group = group_max.get(class_name, None)

            if expected_group is None:
                wrong += 1
            elif expected_group == group_name:
                correct += 1
            else:
                wrong += 1

    accuracy = (correct / total_images) * 100 if total_images > 0 else 0

    print("=== ğŸ“Š Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ ===")
    print(f"ğŸ–¼ï¸ Tá»•ng sá»‘ áº£nh: {total_images}")
    print(f"âœ… áº¢nh Ä‘Ãºng nhÃ³m: {correct}")
    print(f"âŒ áº¢nh sai nhÃ³m: {wrong}")
    print(f"ğŸ¯ Äá»™ chÃ­nh xÃ¡c: {accuracy:.2f}%\n")


    return accuracy


if __name__ == "__main__":
    BASE_DIR = os.path.dirname(os.path.abspath(__file__))
    IMG_DIR = os.path.join(BASE_DIR, "img10")
    FEATURE_FILE = os.path.join(BASE_DIR, "features10.npy")
    NAME_FILE = os.path.join(BASE_DIR, "filenames10.npy")

    # Kiá»ƒm tra vÃ  táº¡o thÆ° má»¥c IMG_DIR náº¿u chÆ°a cÃ³
    if not os.path.exists(IMG_DIR):
        os.makedirs(IMG_DIR)
        print(f"ÄÃ£ táº¡o thÆ° má»¥c {IMG_DIR}. Vui lÃ²ng thÃªm áº£nh vÃ o Ä‘Ã³ vÃ  cháº¡y láº¡i.")
    
    # Gá»i hÃ m chÃ­nh Ä‘á»ƒ thá»±c thi
    features, filenames = mean_extract_image_features_batch_1(
        img_dir=IMG_DIR,
        feature_file=FEATURE_FILE,
        name_file=NAME_FILE,
    )

    print("\n--- HoÃ n táº¥t ---")
    if len(features) > 0:
        print(f"Tá»•ng sá»‘ Ä‘áº·c trÆ°ng Ä‘Ã£ load/trÃ­ch xuáº¥t: {len(features)}")
        print(f"KÃ­ch thÆ°á»›c vector Ä‘áº·c trÆ°ng Ä‘áº§u tiÃªn: {features[0].shape}")
        print(f"TÃªn file Ä‘áº§u tiÃªn: {filenames[0]}")
    else:
        print("KhÃ´ng cÃ³ Ä‘áº·c trÆ°ng nÃ o Ä‘Æ°á»£c xá»­ lÃ½.")

    print("Táº£i dá»¯ liá»‡u feature vÃ  filename...\n")
    features = np.load("features10.npy")
    filenames = np.load("filenames10.npy")

    print("Sá»‘ áº£nh:", len(filenames))
    
    start = time.time()
    build_cluster_faiss(features, filenames, IMG_DIR, "clusters1", threshold=0.8, K=10)  
    end = time.time()

    print(f"[FAISS] Thá»i gian cháº¡y: {end - start:.2f}s\n")
    evaluate_by_image("clusters1")

    ht = HashTable.HashTable(36, features.shape[1])
    ht1 = BloomFilter.BloomFilter(108, features.shape[1],9)
    ht2 = SimHash.SimHash(73) 
    ht3 = MinHash.MinHash(64)

    start = time.time()
    hashtable = build_clusters(ht, features, filenames, IMG_DIR,5,"clusters2")
    end = time.time()
    print(f"[HashTable] Thá»i gian cháº¡y: {end - start:.10f}s\n")
    evaluate_by_image("clusters2")

    start = time.time()
    bloomfilter = build_clusters(ht1, features, filenames, IMG_DIR,21,"clusters3")
    end = time.time()
    print(f"[BloomFilter] Thá»i gian cháº¡y: {end - start:.10f}s\n")
    evaluate_by_image("clusters3")

    start = time.time()
    simhash = build_clusters(ht2,features, filenames, IMG_DIR,13,"clusters4")
    end = time.time()
    print(f"[SimHash] Thá»i gian cháº¡y: {end - start:.2f}s\n")
    evaluate_by_image("clusters4")

    start = time.time()
    minhash = build_clusters(ht3, features, filenames, IMG_DIR,316,"clusters5")
    end = time.time()
    print(f"[MinHash] Thá»i gian cháº¡y: {end - start:.2f}s\n")    
    evaluate_by_image("clusters5")


#     NUM_RUNS = 100

# acc_hash = []
# acc_bloom = []
# acc_simhash = []
# acc_minhash = []

# for i in range(NUM_RUNS):
#     print(f"\n========== ğŸ” Láº§n cháº¡y thá»© {i+1}/{NUM_RUNS} ==========")

#     # --- 1. Khá»Ÿi táº¡o láº¡i tá»«ng cáº¥u trÃºc hash ---
#     ht = HashTable.HashTable(32, features.shape[1])
#     ht1 = BloomFilter.BloomFilter(36, features.shape[1], 9)
#     ht2 = SimHash.SimHash(128)
#     ht3 = MinHash.MinHash(128)

#     # --- 2. Hashtable ---
#     hashtable = build_clusters(ht, features, filenames, IMG_DIR, 5, "clusters")
#     print("[INFO] Hashtable hoÃ n táº¥t clustering.")
#     print(f"Tá»•ng sá»‘ bucket: {len(hashtable)}")
#     _, acc = evaluate_by_image("clusters")
#     acc_hash.append(acc)

#     # --- 3. Bloom Filter ---
#     bloomfilter = build_clusters(ht1, features, filenames, IMG_DIR, 7, "clusters2")
#     print("[INFO] Bloom Filter hoÃ n táº¥t clustering.")
#     print(f"Tá»•ng sá»‘ bucket: {len(bloomfilter)}")
#     _, acc = evaluate_by_image("clusters2")
#     acc_bloom.append(acc)

#     # --- 4. SimHash ---
#     simhash = build_clusters(ht2, features, filenames, IMG_DIR, 13, "clusters3")
#     print("[INFO] SimHash hoÃ n táº¥t clustering.")
#     print(f"Tá»•ng sá»‘ bucket: {len(simhash)}")
#     _, acc = evaluate_by_image("clusters3")
#     acc_simhash.append(acc)

#     # --- 5. MinHash ---
#     minhash = build_clusters(ht3, features, filenames, IMG_DIR, 580, "clusters4")
#     print("[INFO] MinHash hoÃ n táº¥t clustering.")
#     print(f"Tá»•ng sá»‘ bucket: {len(minhash)}")
#     _, acc = evaluate_by_image("clusters4")
#     acc_minhash.append(acc)

# # --- 6. TÃ­nh trung bÃ¬nh accuracy ---
#     print("\n====================== ğŸ“Š Káº¾T QUáº¢ TRUNG BÃŒNH ======================")
#     print(f"Hashtable trung bÃ¬nh: {np.mean(acc_hash):.2f}% Â± {np.std(acc_hash):.2f}")
#     print(f"Bloom Filter trung bÃ¬nh: {np.mean(acc_bloom):.2f}% Â± {np.std(acc_bloom):.2f}")
#     print(f"SimHash trung bÃ¬nh: {np.mean(acc_simhash):.2f}% Â± {np.std(acc_simhash):.2f}")
#     print(f"MinHash trung bÃ¬nh: {np.mean(acc_minhash):.2f}% Â± {np.std(acc_minhash):.2f}")
# def evaluate_by_image(base_dir):
#     label_to_clusters = defaultdict(lambda: defaultdict(int))
#     total_images = 0

#     for group_name in sorted(os.listdir(base_dir)):
#         group_path = os.path.join(base_dir, group_name)
#         if not os.path.isdir(group_path):
#             continue
#         for filename in os.listdir(group_path):
#             if filename.lower().endswith((".jpg", ".jpeg", ".png")):
#                 total_images += 1
#                 label = filename.split("_")[0]
#                 label_to_clusters[label][group_name] += 1

#     label_best_cluster = {
#         label: max(cluster_counts.items(), key=lambda x: x[1])[0]
#         for label, cluster_counts in label_to_clusters.items()
#     }

#     correct = 0
#     for group_name in sorted(os.listdir(base_dir)):
#         group_path = os.path.join(base_dir, group_name)
#         if not os.path.isdir(group_path):
#             continue
#         for filename in os.listdir(group_path):
#             if filename.lower().endswith((".jpg", ".jpeg", ".png")):
#                 label = filename.split("_")[0]
#                 correct_cluster = label_best_cluster[label]
#                 if group_name == correct_cluster:
#                     correct += 1

#     return (correct / total_images * 100) if total_images else 0


# # === HÃ m chÃ­nh: Tuning tá»«ng hash + FAISS trÃªn tá»«ng dataset ===
# def run_tuning():
#     dataset_ids = [1, 2, 3, 5, 10]
#     all_results = {}
#     early_stop_count = 120  # dá»«ng náº¿u acc khÃ´ng cáº£i thiá»‡n 20 láº§n liÃªn tiáº¿p

#     for did in enumerate(tqdm(dataset_ids)) :
#         feature_file = f"features{did}.npy"
#         name_file = f"filenames{did}.npy"
#         IMG_DIR = f"img{did}"

#         # Kiá»ƒm tra dá»¯ liá»‡u
#         if not (os.path.exists(feature_file) and os.path.exists(name_file) and os.path.exists(IMG_DIR)):
#             print(f"\nâš ï¸ Bá» qua dataset {did} â€” thiáº¿u file hoáº·c thÆ° má»¥c áº£nh.")
#             continue

#         print(f"\nğŸ“‚ Dataset {did}: {feature_file}")
#         features = np.load(feature_file)
#         filenames = np.load(name_file)
#         print(f" â†’ {len(filenames)} áº£nh, vector kÃ­ch thÆ°á»›c {features.shape[1]}")

#         dataset_results = {}

#         # === ğŸŸ  FAISS ===
#         print("\nğŸ”¶ [FAISS] Tuning tham sá»‘...")
#         faiss_best = {"acc": 0, "param": None}
#         no_improve = 0
#         for threshold in np.arange(0.7, 0.91, 0.1):
#             for K in range(10, 58, 4):
#                 cluster_dir = f"clusters_faiss"
#                 start = time.time()
#                 build_cluster_faiss(features, filenames, IMG_DIR, cluster_dir, threshold, K)
#                 end = time.time()
#                 acc = evaluate_by_image(cluster_dir)
#                 print(f" threshold={threshold:.2f}, K={K:<3d} â†’ acc={acc:.2f}% ({end - start:.2f}s)")

#                 if acc > faiss_best["acc"]:
#                     faiss_best = {"acc": acc, "param": (threshold, K)}
#                     no_improve = 0
#                 else:
#                     no_improve += 1
#                     if no_improve >= early_stop_count:
#                         print(f" âš ï¸ Early stop: acc khÃ´ng cáº£i thiá»‡n {early_stop_count} láº§n liÃªn tiáº¿p.")
#                         break
#             if no_improve >= early_stop_count:
#                 break
#         dataset_results["FAISS"] = faiss_best

#         # === ğŸ”· HashTable ===
#         print("\nğŸ”· [HashTable] Tuning tham sá»‘...")
#         ht_best = {"acc": 0, "param": None}
#         no_improve = 0
#         for buckets in range(32, 2048, 4):
#             for threshold in range(1, round(buckets/2), 1):
#                 cluster_dir = f"clusters_ht"
#                 ht = HashTable.HashTable(buckets, features.shape[1])
#                 build_clusters(ht, features, filenames, IMG_DIR, threshold, cluster_dir)
#                 acc = evaluate_by_image(cluster_dir)
#                 print(f" buckets={buckets:<4d}, threshold={threshold:<4d} â†’ acc={acc:.2f}%")

#                 if acc > ht_best["acc"]:
#                     ht_best = {"acc": acc, "param": (buckets, threshold)}
#                     no_improve = 0
#                 else:
#                     no_improve += 1
#                     if no_improve >= early_stop_count:
#                         print(f" âš ï¸ Early stop: acc khÃ´ng cáº£i thiá»‡n {early_stop_count} láº§n liÃªn tiáº¿p.")
#                         break
#             if no_improve >= early_stop_count:
#                 break
#         dataset_results["HashTable"] = ht_best

#         # === ğŸŸ¢ BloomFilter ===
#         print("\nğŸŸ¢ [BloomFilter] Tuning tham sá»‘...")
#         bf_best = {"acc": 0, "param": None}
#         no_improve = 0
#         for k_hash in [3, 5, 7, 9, 11]:
#             for bit_size in range(36, 129, 12):
#                 if bit_size % k_hash != 0:
#                     continue
#                 for threshold in range(1, round(bit_size/2), 4):
#                     cluster_dir = f"clusters_bf"
#                     bf = BloomFilter.BloomFilter(bit_size, features.shape[1], k_hash)
#                     build_clusters(bf, features, filenames, IMG_DIR, threshold, cluster_dir)
#                     acc = evaluate_by_image(cluster_dir)
#                     print(f" bits={bit_size:<3d}, k={k_hash:<2d}, threshold={threshold:<4d} â†’ acc={acc:.2f}%")

#                     if acc > bf_best["acc"]:
#                         bf_best = {"acc": acc, "param": (bit_size, k_hash, threshold)}
#                         no_improve = 0
#                     else:
#                         no_improve += 1
#                         if no_improve >= early_stop_count:
#                             print(f" âš ï¸ Early stop: acc khÃ´ng cáº£i thiá»‡n {early_stop_count} láº§n liÃªn tiáº¿p.")
#                             break
#                 if no_improve >= early_stop_count:
#                     break
#             if no_improve >= early_stop_count:
#                 break
#         dataset_results["BloomFilter"] = bf_best

#         # === ğŸŸ£ SimHash ===
#         print("\nğŸŸ£ [SimHash] Tuning tham sá»‘...")
#         sim_best = {"acc": 0, "param": None}
#         no_improve = 0
#         for bits in range(64, 129, 1):
#             for threshold in range(1, 51, 4):
#                 cluster_dir = f"clusters_sim"
#                 sim = SimHash.SimHash(bits)
#                 build_clusters(sim, features, filenames, IMG_DIR, threshold, cluster_dir)
#                 acc = evaluate_by_image(cluster_dir)
#                 print(f" bits={bits:<3d}, threshold={threshold:<3d} â†’ acc={acc:.2f}%")

#                 if acc > sim_best["acc"]:
#                     sim_best = {"acc": acc, "param": (bits, threshold)}
#                     no_improve = 0
#                 else:
#                     no_improve += 1
#                     if no_improve >= early_stop_count:
#                         print(f" âš ï¸ Early stop: acc khÃ´ng cáº£i thiá»‡n {early_stop_count} láº§n liÃªn tiáº¿p.")
#                         break
#             if no_improve >= early_stop_count:
#                 break
#         dataset_results["SimHash"] = sim_best

#         # === ğŸŸ¡ MinHash ===
#         print("\nğŸŸ¡ [MinHash] Tuning tham sá»‘...")
#         min_best = {"acc": 0, "param": None}
#         no_improve = 0
#         for sig_size in range(64, 513, 1):
#             for threshold in range(1, 1001, 4):
#                 cluster_dir = f"clusters_min"
#                 mh = MinHash.MinHash(sig_size)
#                 build_clusters(mh, features, filenames, IMG_DIR, threshold, cluster_dir)
#                 acc = evaluate_by_image(cluster_dir)
#                 print(f" sig_size={sig_size:<3d}, threshold={threshold:<3d} â†’ acc={acc:.2f}%")

#                 if acc > min_best["acc"]:
#                     min_best = {"acc": acc, "param": (sig_size, threshold)}
#                     no_improve = 0
#                 else:
#                     no_improve += 1
#                     if no_improve >= early_stop_count:
#                         print(f" âš ï¸ Early stop: acc khÃ´ng cáº£i thiá»‡n {early_stop_count} láº§n liÃªn tiáº¿p.")
#                         break
#             if no_improve >= early_stop_count:
#                 break
#         dataset_results["MinHash"] = min_best

#         all_results[f"dataset_{did}"] = dataset_results

#     # === Tá»•ng káº¿t ===
#     print("\n===============================")
#     print("ğŸ† Káº¾T QUáº¢ Tá»I Æ¯U TOÃ€N Bá»˜")
#     print("===============================")
#     for ds, results in all_results.items():
#         print(f"\nğŸ“ {ds}:")
#         for method, res in results.items():
#             print(f" {method:<12} â†’ best={res['param']} acc={res['acc']:.2f}%")

#     return all_results
# if __name__ == "__main__":
#     results = run_tuning()