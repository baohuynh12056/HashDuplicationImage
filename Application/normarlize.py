import os
import shutil
import numpy as np
import cv2
from collections import defaultdict
import hash_table_py as HashTable
import simhash_py as SimHash
import minhash_py as MinHash
import bloom_filter_py as BloomFilter
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
IMG_DIR = os.path.join(BASE_DIR, "img")
NUM_BUCKETS = 10  # sá»‘ bucket trong hash table
CLUSTER_DIR = "clusters"
NUM_PLANES = 128

def l2_normalize(vectors):
    norms = np.linalg.norm(vectors, axis=1, keepdims=True)
    return vectors / norms
def merge_similar_buckets(hashtable, threshold=1):
    """
    Gom cÃ¡c bucket cÃ³ hash_value gáº§n nhau (Hamming distance <= threshold).
    Tráº£ vá» list cÃ¡c nhÃ³m (má»—i nhÃ³m lÃ  list tÃªn áº£nh).
    """
    hash_keys = list(hashtable.keys())
    merged = set()
    groups = []

    for i, h1 in enumerate(hash_keys):
        if h1 in merged:
            continue

        # NhÃ³m má»›i báº¯t Ä‘áº§u tá»« bucket hiá»‡n táº¡i
        group = list(hashtable[h1])
        merged.add(h1)

        # So sÃ¡nh vá»›i cÃ¡c bucket khÃ¡c
        for j, h2 in enumerate(hash_keys):
            if h2 in merged:
                continue
            if hamming_distance(h1, h2) <= threshold:
                group.extend(hashtable[h2])
                merged.add(h2)

        groups.append(group)

    return groups
def normalize(features):
    """Chuáº©n hÃ³a vector vá» [0,1]."""
    min_val = features.min(axis=0)
    max_val = features.max(axis=0)
    norm = (features - min_val) / (max_val - min_val + 1e-10)
    return norm
def hamming_distance(a: int, b: int) -> int:
    return bin(a ^ b).count('1')
def evaluate_sharpness(image_path):
    """
    ÄÃ¡nh giÃ¡ Ä‘á»™ sáº¯c nÃ©t (Laplacian Variance). GiÃ¡ trá»‹ cÃ ng cao cÃ ng nÃ©t.
    """
    try:
        img = cv2.imread(image_path)
        if img is None: return 0.0
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        return cv2.Laplacian(gray, cv2.CV_64F).var()
    except Exception:
        return 0.0
def evaluate_colorfulness(image_path):
    """
    ÄÃ¡nh giÃ¡ má»©c Ä‘á»™ mÃ u sáº¯c (Mean Saturation). GiÃ¡ trá»‹ cÃ ng cao cÃ ng nhiá»u mÃ u.
    """
    try:
        img = cv2.imread(image_path)
        if img is None: return 0.0
        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)
        # Láº¥y giÃ¡ trá»‹ trung bÃ¬nh cá»§a kÃªnh Saturation (Äá»™ bÃ£o hÃ²a)
        return np.mean(hsv[:, :, 1])
    except Exception:
        return 0.0
def select_best_image_in_group(image_path1, image_path2, sharp_thresh=100.0, color_thresh=20.0):
    """
    So sÃ¡nh 2 áº£nh vÃ  tráº£ vá» áº£nh tá»‘t hÆ¡n.
    - áº¢nh "tá»‘t" lÃ  áº£nh cÃ³ Ä‘á»™ sáº¯c nÃ©t cao vÃ  mÃ u sáº¯c tá»‘t.
    - Náº¿u cáº£ 2 Ä‘á»u khÃ´ng Ä‘áº¡t ngÆ°á»¡ng, chá»n áº£nh sáº¯c nÃ©t hÆ¡n.
    """
    sharp1 = evaluate_sharpness(image_path1)
    color1 = evaluate_colorfulness(image_path1)

    sharp2 = evaluate_sharpness(image_path2)
    color2 = evaluate_colorfulness(image_path2)

    good1 = (sharp1 > sharp_thresh and color1 > color_thresh)
    good2 = (sharp2 > sharp_thresh and color2 > color_thresh)

    if good1 and good2:
        return (image_path1, "Giá»¯ áº£nh 1") if sharp1 >= sharp2 else (image_path2, "Giá»¯ áº£nh 2")
    elif good1:
        return image_path1, "Giá»¯ áº£nh 1 (áº£nh 2 kÃ©m mÃ u/sáº¯c)"
    elif good2:
        return image_path2, "Giá»¯ áº£nh 2 (áº£nh 1 kÃ©m mÃ u/sáº¯c)"
    else:
        # Cáº£ 2 Ä‘á»u khÃ´ng Ä‘áº¡t ngÆ°á»¡ng â†’ chá»n áº£nh sáº¯c nÃ©t hÆ¡n
        return (image_path1, "Giá»¯ áº£nh 1 (fallback)") if sharp1 >= sharp2 else (image_path2, "Giá»¯ áº£nh 2 (fallback)")      
def build_clusters_hash_table(features, filenames, img_folder):
    """XÃ¢y dá»±ng cluster tá»« feature vectors vÃ  lÆ°u áº£nh theo bucket."""
    hashtable = defaultdict(list)
    ht = HashTable.HashTable(32, features.shape[1])
    normalized_features = l2_normalize(features)
    print("[INFO] Báº¯t Ä‘áº§u thÃªm vector vÃ o HashTable...")
    for i, vec in enumerate(normalized_features):
         # Ã©p sang list náº¿u C++ binding yÃªu cáº§u
        hash_key = ht.hashFunction(vec.tolist())
        print(hash_key,filenames[i])
        hashtable[hash_key].append(filenames[i])

    # XÃ³a thÆ° má»¥c cÅ© náº¿u cÃ³
    if os.path.exists(CLUSTER_DIR):
        shutil.rmtree(CLUSTER_DIR)
    os.makedirs(CLUSTER_DIR)

    # print("Báº¯t Ä‘áº§u lÆ°u áº£nh vÃ o cÃ¡c cá»¥m...")
    # for bucket_id, img_list in hashtable.items():
    #     cluster_path = os.path.join(CLUSTER_DIR, f"bucket_{bucket_id}")
    #     os.makedirs(cluster_path, exist_ok=True)
    #     if not img_list:
    #         continue   
    #     for fname in img_list:
    #         src = os.path.join(img_folder, fname)
    #         if os.path.exists(src):
    #             shutil.copy(src, os.path.join(cluster_path, fname))
    print("ğŸ” Gom nhÃ³m áº£nh theo Ä‘á»™ tÆ°Æ¡ng tá»± hash...")
    groups = merge_similar_buckets(hashtable, threshold=5)
    print(f"âœ… Táº¡o {len(groups)} cá»¥m áº£nh (threshold={5}).")

    print("ğŸ’¾ Báº¯t Ä‘áº§u lÆ°u áº£nh vÃ o cÃ¡c cá»¥m...")
    os.makedirs(CLUSTER_DIR, exist_ok=True)

    for group_id, group in enumerate(groups):
        cluster_path = os.path.join(CLUSTER_DIR, f"group_{group_id:03d}")
        os.makedirs(cluster_path, exist_ok=True)

        for fname in group:
            src = os.path.join(img_folder, fname)
            dst = os.path.join(cluster_path, os.path.basename(fname))
            if os.path.exists(src):
                shutil.copy(src, dst)    
    print(f"ÄÃ£ táº¡o {len(hashtable)} bucket trong thÆ° má»¥c '{CLUSTER_DIR}/'")
    return hashtable, groups

  
def build_clusters_best(features, filenames, img_folder):
    """XÃ¢y dá»±ng cluster tá»« feature vectors vÃ  lÆ°u áº£nh theo bucket."""
    hashtable = defaultdict(list)
    ht = HashTable.HashTable(NUM_BUCKETS, 64, features.shape[1])

    print("Báº¯t Ä‘áº§u thÃªm vector vÃ o HashTable...",flush=True)
    for i, vec in enumerate(features): 
        hash_key = ht.hashFunction(vec.tolist()) 
        print(hash_key)
        img_path = os.path.join(img_folder, filenames[i])
        if not os.path.exists(img_path):
            continue

        # ÄÃ¡nh giÃ¡ áº£nh má»›i
        sharpness = evaluate_sharpness(img_path)
        color_score = evaluate_colorfulness(img_path)

        if hash_key not in hashtable:
            # ChÆ°a cÃ³ áº£nh nÃ o trong bucket nÃ y
            hashtable[hash_key] = {
                'path': img_path,
                'sharpness': sharpness,
                'color': color_score
            }
        else:
            # So sÃ¡nh vá»›i áº£nh hiá»‡n cÃ³
            current = hashtable[hash_key]
            better_path, _ = select_best_image_in_group(
                current['path'], img_path
            )
            # Náº¿u áº£nh má»›i tá»‘t hÆ¡n â†’ thay tháº¿
            if better_path == img_path:
                hashtable[hash_key] = {
                    'path': img_path,
                    'sharpness': sharpness,
                    'color': color_score
                }

    # XÃ³a thÆ° má»¥c cÅ© vÃ  táº¡o má»›i
    if os.path.exists(CLUSTER_DIR):
        shutil.rmtree(CLUSTER_DIR)
    os.makedirs(CLUSTER_DIR, exist_ok=True)

    print("Báº¯t Ä‘áº§u lÆ°u áº£nh tá»‘t nháº¥t vÃ o tá»«ng bucket...")
    for bucket_id, info in hashtable.items():
        cluster_path = os.path.join(CLUSTER_DIR, f"bucket_{bucket_id}")
        os.makedirs(cluster_path, exist_ok=True)
        shutil.copy(info['path'], cluster_path)
        print(info['path'])

    print(f" ÄÃ£ táº¡o {len(hashtable)} bucket, má»—i bucket chá»©a 1 áº£nh tá»‘t nháº¥t.")
    return hashtable
        
def build_clusters_min_hash(features, filenames, img_folder):
    """
    XÃ¢y dá»±ng cluster báº±ng MinHash cho táº­p áº£nh.
    -----------------------------------------
    features: np.ndarray (n_samples, 2048)
        Vector Ä‘áº·c trÆ°ng trÃ­ch tá»« ResNet.
    filenames: List[str]
        TÃªn file áº£nh tÆ°Æ¡ng á»©ng vá»›i tá»«ng vector.
    img_folder: str
        ÄÆ°á»ng dáº«n chá»©a áº£nh gá»‘c.
    """

    print("âš™ï¸  Khá»Ÿi táº¡o MinHash...")
    mh = MinHash.MinHash(NUM_PLANES)

    print("ğŸ“Š Chuáº©n hÃ³a Ä‘áº·c trÆ°ng (L2)...")
    normalized_features = l2_normalize(features)
    n_samples = normalized_features.shape[0]

    print(f"[INFO] Báº¯t Ä‘áº§u tÃ­nh MinHash signatures cho {n_samples} áº£nh...")
    signatures = mh.computeSignatures(normalized_features.tolist(), useMedianThreshold=False)

    # ---- Táº¡o HashTable ----
    hashtable = defaultdict(list)
    for i, sig in enumerate(signatures):
        # DÃ¹ng signature dáº¡ng bit list -> chuyá»ƒn sang tuple Ä‘á»ƒ lÃ m key hashable
        key = int(''.join(map(str, sig)), 2)
        hashtable[key].append(filenames[i])
        print(filenames[i],i)
    print(f"ğŸ” Tá»•ng sá»‘ bucket táº¡o ra: {len(hashtable)}")

    # ---- Merge cÃ¡c bucket tÆ°Æ¡ng tá»± nhau ----
    print("ğŸ”— Gom nhÃ³m áº£nh tÆ°Æ¡ng tá»± theo Hamming distance...")
    groups = merge_similar_buckets(hashtable, threshold=580)
    print(f"âœ… ÄÃ£ táº¡o {len(groups)} cá»¥m áº£nh (threshold = 5).")

    # ---- LÆ°u áº£nh theo nhÃ³m ----
    if os.path.exists(CLUSTER_DIR):
        shutil.rmtree(CLUSTER_DIR)
    os.makedirs(CLUSTER_DIR)

    print("ğŸ’¾ Báº¯t Ä‘áº§u lÆ°u áº£nh vÃ o thÆ° má»¥c cá»¥m...")
    for group_id, group in enumerate(groups):
        cluster_path = os.path.join(CLUSTER_DIR, f"group_{group_id:03d}")
        os.makedirs(cluster_path, exist_ok=True)
        for fname in group:
            src = os.path.join(img_folder, fname)
            dst = os.path.join(cluster_path, os.path.basename(fname))
            if os.path.exists(src):
                shutil.copy(src, dst)

    print(f"ğŸ‰ HoÃ n táº¥t! ÄÃ£ táº¡o {len(groups)} cá»¥m áº£nh trong thÆ° má»¥c '{CLUSTER_DIR}/'.")
    return hashtable

def build_clusters_sim_hash(features, filenames, img_folder, threshold=13):
    """
    features: np.ndarray shape (N, D)
    filenames: list of filenames (len N)
    img_folder: folder path where images are stored
    threshold: hamming-distance threshold to merge buckets
    use_normalize_for_training: náº¿u True thÃ¬ train IDF trÃªn vector Ä‘Ã£ L2-normalized
    """
    print("ğŸ”¹ Chuáº©n bá»‹ dá»¯ liá»‡u...")

    # Báº¡n cÃ³ thá»ƒ train IDF trÃªn dá»¯ liá»‡u gá»‘c hoáº·c trÃªn báº£n chuáº©n hÃ³a â€” tuá»³ chiáº¿n lÆ°á»£c.
    train_features = l2_normalize(features)

    # Chuyá»ƒn sang list Ä‘á»ƒ binding C++ cháº¥p nháº­n
    all_features_list = train_features.tolist()

    # Khá»Ÿi táº¡o SimHash (C++ binding)
    ht = SimHash.SimHash(128) 
    # ----- BÆ¯á»šC QUAN TRá»ŒNG: train IDF -----
    print("[INFO] Huáº¥n luyá»‡n IDF trÃªn toÃ n bá»™ táº­p feature vectors...")
    ht.IDF(all_features_list)   # báº¯t buá»™c gá»i trÆ°á»›c khi hash náº¿u C++ dÃ¹ng TF-IDF

    # Náº¿u báº¡n muá»‘n chuáº©n hÃ³a khi bÄƒm, dÃ¹ng L2-normalize
    normalized_features = l2_normalize(features)

    hashtable = defaultdict(list)
    print("[INFO] Báº¯t Ä‘áº§u tÃ­nh SimHash cho tá»«ng vector...")
    for i, vec in enumerate(normalized_features):
        # chuyá»ƒn sang list cho binding
        feature_list = vec.tolist()
        hash_key = ht.hashFunction(feature_list)
        hashtable[hash_key].append(filenames[i])
        if (i + 1) % 200 == 0:
            print(f"  â†’ ÄÃ£ xá»­ lÃ½ {i+1}/{len(filenames)} áº£nh")

    # XÃ³a vÃ  táº¡o folder káº¿t quáº£
    if os.path.exists(CLUSTER_DIR):
        shutil.rmtree(CLUSTER_DIR)
    os.makedirs(CLUSTER_DIR, exist_ok=True)

    print("ğŸ” Gom nhÃ³m bucket gáº§n nhau theo Hamming distance...")
    groups = merge_similar_buckets(hashtable, threshold=threshold)
    print(f"âœ… ÄÃ£ táº¡o {len(groups)} cá»¥m áº£nh (threshold={threshold}).")

    print("ğŸ’¾ LÆ°u áº£nh vÃ o thÆ° má»¥c cá»¥m...")
    for gid, group in enumerate(groups):
        cluster_path = os.path.join(CLUSTER_DIR, f"group_{gid:03d}")
        os.makedirs(cluster_path, exist_ok=True)
        for fname in group:
            src = os.path.join(img_folder, fname)
            dst = os.path.join(cluster_path, os.path.basename(fname))
            if os.path.exists(src):
                shutil.copy(src, dst)

    print(f"ğŸ¯ HoÃ n táº¥t: {len(hashtable)} hash bucket â†’ {len(groups)} cá»¥m.")
    return hashtable, groups

def build_bloom_clusters(features, filenames, img_folder, threshold =7, cluster_dir=CLUSTER_DIR):
    """
    Gom nhÃ³m áº£nh dá»±a trÃªn Bloom Filter hash.
    - Má»—i áº£nh cÃ³ m hashValues (BloomFilter)
    - Vá»›i má»—i hashValue, thÃªm filename vÃ o hashtable[hashValue]
    - Gom nhÃ³m theo Hamming distance Â±threshold trÃªn tá»«ng hashValue
    """
    ht = BloomFilter.BloomFilter(36, features.shape[1],9)
    normalized_features = l2_normalize(features)

    # 2ï¸âƒ£ TÃ­nh hashValues vÃ  lÆ°u vÃ o hashtable
    hashtable = defaultdict(set)  # key = hashValue, value = list filename
    pairs = defaultdict(list)

    print("[INFO] TÃ­nh hashValues tá»« BloomFilter...")
    for i, vec in enumerate(normalized_features):
        hash_values = ht.hashFunction(vec.tolist())  # list<size_t> vá»›i m hash
        for h in hash_values:
            hashtable[h].add(filenames[i])  # Bloom Filter: Ä‘Ã¡nh dáº¥u tá»«ng hashValue riÃªng
        pairs[filenames[i]] = hash_values
    print(f"âœ… ÄÃ£ Ä‘Ã¡nh dáº¥u {len(pairs)} áº£nh trong BloomFilter.")

    # 3ï¸âƒ£ Gom nhÃ³m áº£nh gáº§n giá»‘ng  
    grouped = set()
    groups = []

    print(f"[INFO] Gom nhÃ³m theo Hamming distance â‰¤ {threshold}...")
    for fname_i in filenames:
        if fname_i in grouped:
            continue  # áº£nh Ä‘Ã£ vÃ o nhÃ³m nÃ o rá»“i thÃ¬ bá» qua
        hash_i = pairs[fname_i]
        group = [fname_i]
        grouped.add(fname_i)

        # Kiá»ƒm tra cÃ¡c áº£nh khÃ¡c dá»±a trÃªn hashtable tá»«ng hashValue
        for h in hash_i:
            for f in hashtable[h]:
                if f not in grouped and f != fname_i:
                    hash_j = pairs[f]
                    total_distance = 0
                    for i in range(9):  # tá»« 0 Ä‘áº¿n 8
                        total_distance += hamming_distance(hash_i[i], hash_j[i])
                    if total_distance <= threshold:
                        group.append(f)
                        grouped.add(f)


        groups.append(group)

    print(f"âœ… Táº¡o {len(groups)} cá»¥m áº£nh (theo threshold={threshold}).")

    # 4ï¸âƒ£ LÆ°u áº£nh ra thÆ° má»¥c
    if os.path.exists(CLUSTER_DIR):
        shutil.rmtree(CLUSTER_DIR)
    os.makedirs(CLUSTER_DIR, exist_ok=True)

    print("[INFO] LÆ°u áº£nh vÃ o thÆ° má»¥c cá»¥m...")
    for gid, group in enumerate(groups):
        cluster_path = os.path.join(cluster_dir, f"group_{gid:03d}")
        os.makedirs(cluster_path, exist_ok=True)
        for fname in group:
            src = os.path.join(img_folder, fname)
            if os.path.exists(src):
                shutil.copy(src, os.path.join(cluster_path, fname))

    print(f"ğŸ’¾ HoÃ n táº¥t â€” {len(groups)} cá»¥m Ä‘Æ°á»£c lÆ°u trong '{cluster_dir}/'")

    return hashtable, groups

def build_clusters(ht, features, filenames, img_folder, threshold=5, cluster_dir= CLUSTER_DIR):
    """
    Gom nhÃ³m áº£nh dá»±a trÃªn loáº¡i hash Ä‘Æ°á»£c truyá»n vÃ o (SimHash, MinHash, BloomFilter, HashTable,...)
    ht: Ä‘á»‘i tÆ°á»£ng hash Ä‘Ã£ khá»Ÿi táº¡o (vd: SimHash.SimHash(128), MinHash.MinHash(32), ...)
    features: np.ndarray (n_samples, feature_dim)
    filenames: list[str] tÃªn file tÆ°Æ¡ng á»©ng
    img_folder: thÆ° má»¥c chá»©a áº£nh gá»‘c
    threshold: ngÆ°á»¡ng Hamming distance Ä‘á»ƒ gá»™p nhÃ³m
    cluster_dir: nÆ¡i lÆ°u káº¿t quáº£ cÃ¡c nhÃ³m
    """
    normalized_features = l2_normalize(features)
    n_samples = len(filenames)

    print(f"Báº¯t Ä‘áº§u hashing cho {n_samples} áº£nh...")

    #kiá»ƒm tra thuá»™c tÃ­nh cá»§a ht
    if type(ht).__name__ == "SimHash": 
        hashtable = defaultdict(list)
        print("SimHash")
        ht.IDF(normalized_features.tolist())
        for i, vec in enumerate(normalized_features):
            h = ht.hashFunction(vec.tolist())
            hashtable[h].append(filenames[i])

    elif type(ht).__name__ == "MinHash":
        hashtable = defaultdict(list)
        print("MinHash")
        signatures = ht.computeSignatures(normalized_features.tolist(), useMedianThreshold=False)
        for i, sig in enumerate(signatures):
            key = int(''.join(map(str, sig)), 2)
            hashtable[key].append(filenames[i])

    elif type(ht).__name__ == "BloomFilter":
        hashtable = defaultdict(set)
        print("BloomFilter")
        pairs = defaultdict(list)
        for i, vec in enumerate(normalized_features):
            hash_values = ht.hashFunction(vec.tolist())  
            pairs[filenames[i]] = hash_values
            for hv in hash_values:
                hashtable[hv].add(filenames[i])

        grouped = set()
        groups = []
        for fname_i in filenames:
            if fname_i in grouped:
                continue
            hash_i = pairs[fname_i]
            group = [fname_i]
            grouped.add(fname_i)
            for hv in hash_i:
                for f in hashtable[hv]:
                    if f not in grouped and f != fname_i:
                        hash_j = pairs[f]
                        total_dist = sum(hamming_distance(hash_i[t], hash_j[t]) for t in range(len(hash_i)))
                        if total_dist <= threshold:
                            group.append(f)
                            grouped.add(f)
            groups.append(group)
        final_groups = groups

    elif type(ht).__name__ == "HashTable":
        hashtable = defaultdict(list)
        print("HashTable")
        for i, vec in enumerate(normalized_features):
            h = ht.hashFunction(vec.tolist())
            hashtable[h].append(filenames[i])
        final_groups = merge_similar_buckets(hashtable, threshold)

    else:
        raise ValueError("KhÃ´ng nháº­n diá»‡n Ä‘Æ°á»£c loáº¡i hash Ä‘Æ°á»£c truyá»n vÃ o!")

    if not 'final_groups' in locals():  # trÆ°á»ng há»£p SimHash, MinHash
        final_groups = merge_similar_buckets(hashtable, threshold)

    if os.path.exists(cluster_dir):
        shutil.rmtree(cluster_dir)
    os.makedirs(cluster_dir, exist_ok=True)

    print(f"LÆ°u {len(final_groups)} cá»¥m áº£nh vÃ o thÆ° má»¥c '{cluster_dir}'...")
    for gid, group in enumerate(final_groups):
        cluster_path = os.path.join(cluster_dir, f"group_{gid:03d}")
        os.makedirs(cluster_path, exist_ok=True)
        for fname in group:
            src = os.path.join(img_folder, fname)
            if os.path.exists(src):
                shutil.copy(src, os.path.join(cluster_path, os.path.basename(fname)))

    print(f"HoÃ n táº¥t! {len(hashtable)} bucket , {len(final_groups)} cá»¥m.")
    return hashtable, final_groups